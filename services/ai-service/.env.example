# CS720 AI Service Configuration

# Server
PORT=3003
NODE_ENV=development

# LLM Backend Configuration
PREFERRED_BACKEND=ollama  # ollama | openai

# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# OpenAI-Compatible Configuration (via Proxy)
PROXY_URL=http://localhost:3002
OPENAI_ENDPOINT=https://api.openai.com/v1
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=gpt-4

# AI Parameters
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=2048
